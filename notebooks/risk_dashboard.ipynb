{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboards and Final Report\n",
    "\n",
    "This notebook builds interactive dashboards and generates a textual summary\n",
    "report for the **Yelp Rating Drop Early-Warning System** using the **optimized\n",
    "Random Forest model**.\n",
    "\n",
    "It uses the following optimized artifacts:\n",
    "\n",
    "- `data/processed/reviews_features_optimized.csv`\n",
    "- `models/rating_drop_model_optimized.pkl`\n",
    "- `models/feature_list_optimized.txt`\n",
    "- `models/model_summary_optimized.csv`\n",
    "- `results/llm_complaint_analysis.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and output directories ensured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(\"dashboards\", exist_ok=True)\n",
    "os.makedirs(\"reports\", exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported and output directories ensured.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Optimized Data and Model Artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded optimized features from: data/processed/reviews_features_optimized.csv\n",
      "Shape: (89878, 41)\n",
      "Loaded optimized model from: models/rating_drop_model_optimized.pkl\n",
      "Loaded 31 features from: models/feature_list_optimized.txt\n",
      "Using 31 features present in dataset.\n",
      "Loaded model summary from: models/model_summary_optimized.csv\n",
      "Loaded LLM complaint analysis from: results/llm_complaint_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "# Load optimized feature dataset\n",
    "features_path = \"data/processed/reviews_features_optimized.csv\"\n",
    "df = pd.read_csv(features_path, parse_dates=[\"date\"])\n",
    "print(f\"Loaded optimized features from: {features_path}\")\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "# Load optimized model\n",
    "model_path = \"models/rating_drop_model_optimized.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "print(f\"Loaded optimized model from: {model_path}\")\n",
    "\n",
    "# Load optimized feature list\n",
    "feature_list_path = \"models/feature_list_optimized.txt\"\n",
    "with open(feature_list_path, \"r\") as f:\n",
    "    feature_cols = [line.strip() for line in f.readlines() if line.strip()]\n",
    "print(f\"Loaded {len(feature_cols)} features from: {feature_list_path}\")\n",
    "\n",
    "# Intersect with available columns just in case\n",
    "feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "print(f\"Using {len(feature_cols)} features present in dataset.\")\n",
    "\n",
    "# Load model summary\n",
    "model_summary_path = \"models/model_summary_optimized.csv\"\n",
    "model_summary = pd.read_csv(model_summary_path).iloc[0]\n",
    "print(f\"Loaded model summary from: {model_summary_path}\")\n",
    "\n",
    "# Load LLM complaint analysis if available\n",
    "llm_path = \"results/llm_complaint_analysis.csv\"\n",
    "llm_df = None\n",
    "if os.path.exists(llm_path):\n",
    "    llm_df = pd.read_csv(llm_path)\n",
    "    print(f\"Loaded LLM complaint analysis from: {llm_path}\")\n",
    "    if \"date\" in llm_df.columns:\n",
    "        llm_df[\"date\"] = pd.to_datetime(llm_df[\"date\"], errors=\"coerce\")\n",
    "else:\n",
    "    print(\n",
    "        \"LLM complaint analysis file not found; some dashboard elements will be skipped.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Risk Scores Using the Optimized Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed risk scores and risk_flag using optimal threshold: 0.29\n",
      "              business_id                                   name  \\\n",
      "0  -0M0b-XhtFagyLmsBtOe8w                         Paris Wine Bar   \n",
      "1  -0TffRSXXIlBYVbb5AwfTg  IndeBlue Modern Indian Food & Spirits   \n",
      "2  -0TffRSXXIlBYVbb5AwfTg  IndeBlue Modern Indian Food & Spirits   \n",
      "3  -0TffRSXXIlBYVbb5AwfTg  IndeBlue Modern Indian Food & Spirits   \n",
      "4  -0TffRSXXIlBYVbb5AwfTg  IndeBlue Modern Indian Food & Spirits   \n",
      "\n",
      "                 date  stars_business  risk_score  risk_flag  \n",
      "0 2015-06-24 22:10:39             3.5    0.426102          1  \n",
      "1 2013-06-27 13:44:37             4.5    0.223922          0  \n",
      "2 2013-08-03 13:56:04             4.5    0.232762          0  \n",
      "3 2013-09-15 17:18:13             4.5    0.187257          0  \n",
      "4 2013-09-18 02:20:26             4.5    0.204912          0  \n"
     ]
    }
   ],
   "source": [
    "# Build feature matrix and compute risk scores\n",
    "X = df[feature_cols].copy()\n",
    "risk_proba = model.predict_proba(X)[:, 1]\n",
    "df[\"risk_score\"] = risk_proba\n",
    "\n",
    "# Attach simple risk flag using optimal threshold from Notebook 2\n",
    "optimal_threshold = float(model_summary.get(\"optimal_threshold\", 0.29))\n",
    "df[\"risk_flag\"] = (df[\"risk_score\"] >= optimal_threshold).astype(int)\n",
    "\n",
    "print(\"Computed risk scores and risk_flag using optimal threshold:\", optimal_threshold)\n",
    "\n",
    "# Basic sanity check\n",
    "print(\n",
    "    df[\n",
    "        [\"business_id\", \"name\", \"date\", \"stars_business\", \"risk_score\", \"risk_flag\"]\n",
    "    ].head()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Business-Level Aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business-level summary shape: (4094, 7)\n",
      "                 business_id                         name  avg_rating  \\\n",
      "1755  QX6qD3JEopZSH2SQsmObxg                Xi'an Cuisine         3.0   \n",
      "3136  kr6FpW5rMNp_OX8Ha4j0Fw  Amasi Restaurant and Hookah         3.5   \n",
      "2824  gU39t80dXx9LkIZHp6cyCQ                Magic Noodles         3.5   \n",
      "366   4zEqYybRD1FQssLGaJnNZA                 The Blockley         3.5   \n",
      "2455  b1pdnbYvBdtTxJOOom5WmA               Andy's Chicken         4.0   \n",
      "\n",
      "      latest_rating  avg_risk_score  max_risk_score  n_reviews  \n",
      "1755            3.0        0.616899        0.645203          6  \n",
      "3136            3.5        0.602955        0.602955          1  \n",
      "2824            3.5        0.600151        0.640090          4  \n",
      "366             3.5        0.585570        0.585570          1  \n",
      "2455            4.0        0.575911        0.626122          6  \n"
     ]
    }
   ],
   "source": [
    "# Aggregate by business\n",
    "business_group = df.groupby([\"business_id\", \"name\"], as_index=False).agg(\n",
    "    avg_rating=(\"stars_business\", \"mean\"),\n",
    "    latest_rating=(\"stars_business\", \"last\"),\n",
    "    avg_risk_score=(\"risk_score\", \"mean\"),\n",
    "    max_risk_score=(\"risk_score\", \"max\"),\n",
    "    n_reviews=(\"review_id\", \"count\"),\n",
    ")\n",
    "\n",
    "# Sort by average risk score (high to low)\n",
    "business_group = business_group.sort_values(\"avg_risk_score\", ascending=False)\n",
    "\n",
    "print(\"Business-level summary shape:\", business_group.shape)\n",
    "print(business_group.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Overview Dashboard\n",
    "\n",
    "This dashboard provides high-level views of risk scores and ratings over time.\n",
    "The HTML is saved in the `dashboards/` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved overview dashboard to: dashboards/dashboard_overview.html\n"
     ]
    }
   ],
   "source": [
    "# Time series of average rating and average risk\n",
    "daily_summary = df.groupby(\"date\", as_index=False).agg(\n",
    "    avg_rating=(\"stars_business\", \"mean\"), avg_risk=(\"risk_score\", \"mean\")\n",
    ")\n",
    "\n",
    "fig_overview = go.Figure()\n",
    "fig_overview.add_trace(\n",
    "    go.Scatter(\n",
    "        x=daily_summary[\"date\"],\n",
    "        y=daily_summary[\"avg_rating\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Average Rating\",\n",
    "    )\n",
    ")\n",
    "fig_overview.add_trace(\n",
    "    go.Scatter(\n",
    "        x=daily_summary[\"date\"],\n",
    "        y=daily_summary[\"avg_risk\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Average Risk Score\",\n",
    "        yaxis=\"y2\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_overview.update_layout(\n",
    "    title=\"Overall Trends: Average Rating vs. Average Risk Score\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Average Rating\",\n",
    "    yaxis2=dict(title=\"Average Risk Score\", overlaying=\"y\", side=\"right\"),\n",
    "    legend=dict(orientation=\"h\"),\n",
    ")\n",
    "\n",
    "overview_path = \"dashboards/dashboard_overview.html\"\n",
    "fig_overview.write_html(overview_path)\n",
    "print(f\"✓ Saved overview dashboard to: {overview_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Alerts Dashboard\n",
    "\n",
    "This dashboard highlights the top high-risk restaurants based on their\n",
    "average risk score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved alerts dashboard to: dashboards/dashboard_alerts.html\n"
     ]
    }
   ],
   "source": [
    "top_n = 20\n",
    "top_risk_businesses = business_group.head(top_n).copy()\n",
    "top_risk_businesses[\"rank\"] = np.arange(1, len(top_risk_businesses) + 1)\n",
    "\n",
    "fig_alerts = px.bar(\n",
    "    top_risk_businesses,\n",
    "    x=\"avg_risk_score\",\n",
    "    y=\"name\",\n",
    "    orientation=\"h\",\n",
    "    hover_data=[\"latest_rating\", \"n_reviews\"],\n",
    "    labels={\"avg_risk_score\": \"Average Risk Score\", \"name\": \"Business\"},\n",
    "    title=f\"Top {top_n} High-Risk Restaurants\",\n",
    ")\n",
    "fig_alerts.update_layout(yaxis=dict(autorange=\"reversed\"))\n",
    "\n",
    "alerts_path = \"dashboards/dashboard_alerts.html\"\n",
    "fig_alerts.write_html(alerts_path)\n",
    "print(f\"✓ Saved alerts dashboard to: {alerts_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LLM Complaint Analysis Dashboard\n",
    "\n",
    "If LLM complaint analysis is available, we summarize complaint categories\n",
    "and severity distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved LLM complaint dashboard to: dashboards/dashboard_llm_complaints.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/ln88kc6d3qn6v_mv1kg4krsc0000gn/T/ipykernel_20918/4176902861.py:13: FutureWarning:\n",
      "\n",
      "DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm_dashboard_path = None\n",
    "\n",
    "if llm_df is not None and len(llm_df) > 0:\n",
    "    # Category columns used in the LLM analysis\n",
    "    category_cols = [\n",
    "        \"food_quality\",\n",
    "        \"service_speed\",\n",
    "        \"staff_behavior\",\n",
    "        \"cleanliness\",\n",
    "        \"portion_size\",\n",
    "        \"pricing\",\n",
    "        \"order_accuracy\",\n",
    "    ]\n",
    "    available_cols = [c for c in category_cols if c in llm_df.columns]\n",
    "\n",
    "    # Compute category counts (treat True as 1, others as 0)\n",
    "    if available_cols:\n",
    "        category_flags = llm_df[available_cols].applymap(\n",
    "            lambda v: 1 if v is True else 0\n",
    "        )\n",
    "        category_counts = category_flags.sum().sort_values(ascending=False)\n",
    "        category_df = category_counts.reset_index()\n",
    "        category_df.columns = [\"category\", \"count\"]\n",
    "\n",
    "        fig_categories = px.bar(\n",
    "            category_df,\n",
    "            x=\"category\",\n",
    "            y=\"count\",\n",
    "            title=\"LLM Complaint Category Distribution\",\n",
    "            labels={\"category\": \"Category\", \"count\": \"Number of Reviews\"},\n",
    "        )\n",
    "\n",
    "        # Severity distribution if available\n",
    "        severity_fig = None\n",
    "        if \"severity\" in llm_df.columns:\n",
    "            sev_counts = llm_df[\"severity\"].value_counts().reset_index()\n",
    "            sev_counts.columns = [\"severity\", \"count\"]\n",
    "            severity_fig = px.bar(\n",
    "                sev_counts,\n",
    "                x=\"severity\",\n",
    "                y=\"count\",\n",
    "                title=\"Complaint Severity Distribution\",\n",
    "                labels={\"severity\": \"Severity\", \"count\": \"Number of Reviews\"},\n",
    "            )\n",
    "\n",
    "        # Combine into a simple HTML report\n",
    "        from plotly.subplots import make_subplots\n",
    "\n",
    "        if severity_fig is not None:\n",
    "            fig_llm = make_subplots(\n",
    "                rows=1,\n",
    "                cols=2,\n",
    "                subplot_titles=(\"Category Distribution\", \"Severity Distribution\"),\n",
    "            )\n",
    "            for trace in fig_categories.data:\n",
    "                fig_llm.add_trace(trace, row=1, col=1)\n",
    "            for trace in severity_fig.data:\n",
    "                fig_llm.add_trace(trace, row=1, col=2)\n",
    "\n",
    "            fig_llm.update_layout(title_text=\"LLM Complaint Analysis Summary\")\n",
    "        else:\n",
    "            fig_llm = fig_categories\n",
    "\n",
    "        llm_dashboard_path = \"dashboards/dashboard_llm_complaints.html\"\n",
    "        fig_llm.write_html(llm_dashboard_path)\n",
    "        print(f\"✓ Saved LLM complaint dashboard to: {llm_dashboard_path}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"No boolean complaint category columns found in LLM output; skipping category dashboard.\"\n",
    "        )\n",
    "else:\n",
    "    print(\"LLM complaint analysis not available; skipping LLM dashboard.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Textual Dashboard Summary (for Reports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved dashboard summary report to: reports/4_dashboard.txt\n"
     ]
    }
   ],
   "source": [
    "report_lines = []\n",
    "\n",
    "report_lines.append(\"=\" * 70)\n",
    "report_lines.append(\"DASHBOARD & BUSINESS INSIGHTS SUMMARY (OPTIMIZED MODEL)\")\n",
    "report_lines.append(\"=\" * 70)\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Model performance summary\n",
    "report_lines.append(\"Model Performance (Optimized Random Forest):\")\n",
    "report_lines.append(f\"  ROC AUC   : {model_summary.get('roc_auc', np.nan):.3f}\")\n",
    "report_lines.append(f\"  Accuracy  : {model_summary.get('accuracy', np.nan):.3f}\")\n",
    "report_lines.append(f\"  Precision : {model_summary.get('precision', np.nan):.3f}\")\n",
    "report_lines.append(f\"  Recall    : {model_summary.get('recall', np.nan):.3f}\")\n",
    "report_lines.append(f\"  F1-Score  : {model_summary.get('f1_score', np.nan):.3f}\")\n",
    "report_lines.append(f\"  Threshold : {optimal_threshold:.3f}\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Overall stats\n",
    "n_businesses = business_group[\"business_id\"].nunique()\n",
    "n_reviews = len(df)\n",
    "n_high_risk = (business_group[\"avg_risk_score\"] >= optimal_threshold).sum()\n",
    "\n",
    "report_lines.append(\"Overall System Statistics:\")\n",
    "report_lines.append(f\"  Businesses monitored : {n_businesses}\")\n",
    "report_lines.append(f\"  Reviews analyzed     : {n_reviews}\")\n",
    "report_lines.append(f\"  High-risk businesses : {n_high_risk}\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Top high-risk businesses\n",
    "top_display = 10\n",
    "report_lines.append(f\"Top {top_display} high-risk businesses:\")\n",
    "for _, row in business_group.head(top_display).iterrows():\n",
    "    line = (\n",
    "        f\"  - {row['name']} | \"\n",
    "        f\"avg_risk={row['avg_risk_score']:.3f}, \"\n",
    "        f\"latest_rating={row['latest_rating']:.2f}, \"\n",
    "        f\"reviews={int(row['n_reviews'])}\"\n",
    "    )\n",
    "    report_lines.append(line)\n",
    "\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# LLM insights summary (if available)\n",
    "if llm_df is not None and len(llm_df) > 0:\n",
    "    report_lines.append(\"LLM Complaint Analysis (sampled negative reviews):\")\n",
    "    n_llm = len(llm_df)\n",
    "    report_lines.append(f\"  Reviews analyzed by LLM : {n_llm}\")\n",
    "\n",
    "    if \"primary_issue\" in llm_df.columns:\n",
    "        top_issues = llm_df[\"primary_issue\"].value_counts().head(5)\n",
    "        report_lines.append(\"  Top primary issues:\")\n",
    "        for issue, count in top_issues.items():\n",
    "            if pd.isna(issue) or not str(issue).strip():\n",
    "                continue\n",
    "            pct = (count / n_llm) * 100\n",
    "            report_lines.append(f\"    • {issue} ({int(count)} reviews, {pct:4.1f}%)\")\n",
    "else:\n",
    "    report_lines.append(\"LLM complaint analysis not available in this run.\")\n",
    "\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Save report\n",
    "dashboard_report_path = \"reports/4_dashboard.txt\"\n",
    "with open(dashboard_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(report_lines))\n",
    "\n",
    "print(f\"✓ Saved dashboard summary report to: {dashboard_report_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DASHBOARDS & REPORT GENERATION COMPLETE (OPTIMIZED PIPELINE)\n",
      "======================================================================\n",
      "\n",
      "Generated files:\n",
      "  - dashboards/dashboard_overview.html\n",
      "  - dashboards/dashboard_alerts.html\n",
      "  - dashboards/dashboard_llm_complaints.html (if LLM results available)\n",
      "  - reports/4_dashboard.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DASHBOARDS & REPORT GENERATION COMPLETE (OPTIMIZED PIPELINE)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - dashboards/dashboard_overview.html\")\n",
    "print(\"  - dashboards/dashboard_alerts.html\")\n",
    "print(\"  - dashboards/dashboard_llm_complaints.html (if LLM results available)\")\n",
    "print(\"  - reports/4_dashboard.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
